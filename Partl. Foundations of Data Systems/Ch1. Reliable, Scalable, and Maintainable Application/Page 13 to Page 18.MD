
# Chapter 1: Page 7 to Page 13
## Understanding System Performance and Response Time Metrics
## üìä Performance Under Load

This section of *Designing Data-Intensive Applications* discusses how to evaluate a system‚Äôs performance when the load changes. The author presents two key perspectives:

1. **Performance Under Fixed Resources** ‚Äì If system resources (CPU, memory, network bandwidth, etc.) remain the same but the load increases, how does performance degrade?
2. **Scaling Resources to Maintain Performance** ‚Äì If the load increases, how much do we need to scale up resources to keep the performance stable?

To answer these questions, performance metrics are needed. The book introduces two key metrics based on the system type:

- **Batch Processing Systems (e.g., Hadoop):** The most relevant metric is *throughput*, which refers to the number of records processed per second or the total time taken to complete a job on a dataset.
- **Online Systems:** The focus is on *response time*, which is the time taken from when a client sends a request to when they receive a response.
---
## üïí Latency vs. Response Time

Although "latency" and "response time" are often used interchangeably, they have distinct meanings:

### üìù Response Time:
The total time a client waits for a response after sending a request. It includes:
- **Service Time:** The actual time needed to process the request.
- **Network Delays:** The time taken for data to travel between the client and server.
- **Queueing Delays:** The waiting time before the request gets processed if the system is busy.

### ‚ö° Latency:
The time a request spends waiting before it starts being processed. It does not include processing time‚Äîjust the waiting period.


### üí° Why Response Time Varies

Even if you send the same request multiple times, response time can fluctuate. This variation becomes even more noticeable in systems handling different types of requests. Instead of thinking of response time as a fixed value, we should consider it as a *distribution of values*.

#### Causes of Response Time Variation

- Some requests are inherently slower because they process more data.
- Even requests that should take the same time can experience delays due to:
  - Context switches (CPU switching to a background process).
  - Network issues (lost packets requiring retransmission).
  - Garbage collection pauses in memory management.
  - Page faults (data being loaded from disk).
  - Physical factors like vibrations affecting server hardware.

---
## Understanding Response Time Metrics

When measuring the performance of a system, it's common to report the *average response time*. However, using the *average (mean)* is not always the best way to understand how long users actually wait. Let's break this down step by step.

### 1. Why is the Average (Mean) Not Always Useful?

The **average (mean)** response time is calculated as:

\[
\text{Mean} = \frac{\text{Sum of all response times}}{\text{Number of requests (n)}}
\]

For example, if you have five response times:

\[
[100ms, 150ms, 200ms, 10,000ms, 50ms]
\]

The mean (average) response time would be:

\[
\frac{100 + 150 + 200 + 10,000 + 50}{5} = \frac{10,500}{5} = 2,100ms
\]

This result of **2,100ms** doesn't represent the typical user experience because one extremely slow request (10,000ms) has heavily skewed the average.

### Why is this misleading?

- The **mean** can be **distorted by outliers** (extremely slow or fast requests).
- The **average** does not show **what most users experience**. Most users may experience much lower response times, but the slowest requests create a higher average.

In this case, even though the average response time is 2,100ms, most users actually experienced much faster responses, closer to the 100ms - 200ms range.

This number (2,100 ms) does not represent what most users experience! Most users actually wait much less than 2,100 ms, but one extremely slow request (10,000 ms) made the average very high.

Thus, the mean is not a good indicator of typical response time because:

- It is affected by outliers (very slow requests).
- It does not tell us how many users actually experienced a delay.

### 2. Using Percentiles Instead of the Mean

A better way to measure response time is using *percentiles*, which help us understand how response times are distributed.

#### What is a Percentile?

- If you sort all response times from fastest to slowest, the **median (50th percentile, p50)** is the middle value.
- The **p50 response time** means that *50% of requests were faster than this value, and 50% were slower.*

##### Example:

If the response times are: `[50ms, 100ms, 150ms, 200ms, 10,000ms]`

- The **median (p50)** is `150ms` because two requests were faster, two were slower.
- This means half the users waited less than `150ms`, and half waited more than `150ms`.

#### Higher Percentiles (p90, p95, p99)

- **90th percentile (p90):** 90% of requests are faster than this value, and 10% are slower.
- **95th percentile (p95):** 95% of requests are faster, and only 5% are slower.
- **99th percentile (p99):** Only 1% of requests are slower than this.

üëâ **Why are these useful?**

- **p50 (median)** tells us what most users experience.
- **p90/p95** tells us what experience the slowest 10% or 5% of users have.
- **p99** shows the worst-case scenario for the slowest 1% of users.

##### Example:

Imagine a website with the following response times (sorted):

`[100ms, 120ms, 150ms, 180ms, 200ms, 500ms, 1000ms, 5000ms, 10000ms]`

- **p50 (median) = 180ms** ‚Üí Half of users get responses in less than `180ms`.
- **p90 = 5000ms** ‚Üí 90% of users get responses in less than `5000ms`.
- **p99 = 10000ms** ‚Üí 99% of users get responses in less than `10,000ms`, but 1% wait this long!

### 3. The Median (p50) for Single Requests vs. Multiple Requests

The **median (p50)** applies to a single request. But in reality, a user may make multiple requests (e.g., loading a webpage requires multiple API calls).

If a user makes several requests per session, there is a higher chance that at least one of them is slower than the median.

For example:

- If the **p50** is `200ms`, it means that **50% of requests are slower than 200ms**.
- If a webpage loads 10 resources, the chance that at least one resource takes longer than `200ms` is much higher than 50%.

Thus, **users may perceive performance worse than the median suggests** because they often load multiple elements at once.

### Summary

‚úÖ The **mean (average) response time is misleading** because outliers (very slow requests) distort it.

‚úÖ **Percentiles are better for understanding real user experience:**

- **p50 (median)** ‚Üí Typical response time for most users.
- **p90/p95** ‚Üí How bad it is for the slowest users.
- **p99** ‚Üí Worst-case scenario.

‚úÖ **Users making multiple requests per session are more likely to experience delays**, even if the median is low.

---

## Understanding High Percentiles and Their Impact on Performance

In system performance analysis, high percentiles (p95, p99, p999) help identify outliers‚Äîthe slowest requests that impact user experience. Let's break this down step by step.

### 1. What Do High Percentiles Represent?

A **percentile** represents the percentage of requests that are faster than a given response time.

- **p95 (95th percentile):** 95% of requests are faster than this value, while 5% are slower.
- **p99 (99th percentile):** 99% of requests are faster, while 1% are slower.
- **p999 (99.9th percentile):** 99.9% of requests are faster, while only 0.1% (1 in 1,000) are slower.

#### üëâ Example Interpretation:

If the **p95** response time is **1.5 seconds**, this means:

- 95 out of 100 requests take less than **1.5 seconds**.
- 5 out of 100 requests take **1.5 seconds or more**.

These high percentiles help analyze the worst-performing requests, often referred to as **tail latencies**.

### 2. Why Are High Percentiles (Tail Latencies) Important?

Even though only a small percentage of users experience high latency, they can be some of the most valuable customers.

#### üîπ Example: Amazon's 99.9th Percentile Focus

Amazon monitors the **99.9th percentile (p999)** because customers with the slowest requests are often those who buy the most (large accounts with more data). 

- Keeping these customers happy prevents revenue loss and maintains customer loyalty.

#### üìå Real-World Impact of Slow Response Times:

- **Amazon observed:** A **100 ms delay** in response time reduces sales by **1%**.
- **Other studies:** A **1-second slowdown** reduces customer satisfaction by **16%**.

#### üîπ Why Not Optimize for p9999 (99.99th Percentile)?

The slowest **1 in 10,000 requests** are often affected by random, uncontrollable factors (e.g., network issues, system spikes).

- Optimizing these extreme cases is **too expensive** and provides little benefit.

### 3. Summary & Key Takeaways

- ‚úÖ **p95, p99, and p999** are useful for detecting slow requests (outliers).
- ‚úÖ Improving **high percentiles** directly improves **user experience**.
- ‚úÖ Companies like **Amazon** prioritize the **99.9th percentile** because slow responses impact their highest-value customers.
- ‚úÖ Beyond a certain point (e.g., **p9999**), optimization is **costly** and has diminishing returns.


---
## Understanding Service Level Objectives (SLOs) and Service Level Agreements (SLAs)

### 1. What Are SLAs and SLOs?

- **Service Level Agreement (SLA):** A contract that defines performance expectations between a service provider and its clients.
- **Service Level Objective (SLO):** A specific performance goal within the SLA that defines what "acceptable" performance is.

#### üí° Example of an SLA:
A company guarantees that:
- ‚úÖ **Median response time** must be under **200 ms**.
- ‚úÖ **99th percentile response time** must be under **1 second**.
- ‚úÖ The service must be **available 99.9%** of the time.

üìå **If these conditions aren‚Äôt met, customers may demand a refund.**

---

### 2. Impact of Queueing Delays on High Percentiles

#### üîπ What Causes Queueing Delays?
- A server has limited processing power (e.g., limited CPU cores).
- If multiple slow requests are processed first, **faster requests** get stuck waiting.
- This issue is known as **Head-of-Line Blocking**‚Äîeven fast requests appear slow because they wait behind slow ones.

#### üîπ Why Is Measuring Response Time on the Client Side Important?
- The server might complete requests quickly, but queueing effects make the **actual response time** seen by users much worse.
- **Measuring on the client side** gives a true picture of what users experience.

---

### 3. How Load Testing Can Skew Measurements

When testing system scalability, we often generate fake load to see how the system behaves. However:

- ‚ùå If the test client waits for each request to finish before sending the next, it **artificially shortens the queue**.
- ‚úÖ A realistic test client should send requests **continuously**, regardless of whether previous ones have finished.

#### üí° Why?
- In the real world, users don‚Äôt wait for others to finish before making new requests.
- This ensures **accurate load testing** and helps predict real-world performance.

---

### 4. Summary & Key Takeaways

- ‚úÖ **SLAs & SLOs** define performance expectations (e.g., max response times, uptime).
- ‚úÖ **Queueing delays** and **head-of-line blocking** slow down response times, even for fast requests.
- ‚úÖ **Measuring response time on the client side** is critical to seeing **real user experience**.
- ‚úÖ **Load testing** must simulate **real-world request patterns** to avoid misleading results.

---
## Understanding Percentiles in Backend Services

### 1. Why Are High Percentiles Important in Backend Services?
Backend services often need to handle multiple requests per user action. These requests may be made in parallel, but the end-user still has to wait for the **slowest request** to finish.

#### üí° Example:
Imagine a mobile app making 5 backend calls to load a page:
- ‚úÖ 4 calls finish in **100 ms**.
- ‚ùå 1 call takes **2 seconds**.
- üö® The user must wait **2 seconds**‚Äîeven though most calls were fast!

This problem is called **Tail Latency Amplification** because even a small percentage of slow requests can significantly affect user experience.

---

### 2. How to Monitor Response Time Percentiles?
To monitor system performance, you should track **response times continuously** and update percentiles dynamically.

#### A Common Approach:
- üìå **Maintain a rolling window** (e.g., the last 10 minutes of request times).
- üìå Every minute, calculate:
  - The **median** (p50).
  - Other percentiles (p95, p99, etc.).
- üìå **Plot these metrics** on a dashboard for real-time monitoring.

---

### 3. Efficiently Calculating Percentiles
A na√Øve way to compute percentiles is:
1Ô∏è‚É£ **Store all response times** for the last 10 minutes.
2Ô∏è‚É£ **Sort them** every minute.
3Ô∏è‚É£ **Pick the required percentiles.**

‚ùå **Problem:** Sorting is slow and inefficient!

‚úÖ **Better Solution:** Use specialized approximation algorithms like:
- **Forward Decay**
- **t-Digest**
- **HdrHistogram**

---

### 4. Common Mistakes: Don't Average Percentiles!
A common mistake is averaging percentiles to reduce time resolution or combine data from multiple servers.

#### ‚ùå Incorrect Approach:
- (p95 of server A + p95 of server B) / 2

#### ‚úÖ Correct Approach:
- Instead of averaging, **combine histograms** from different servers and recalculate percentiles.

---

### 5. Key Takeaways
- ‚úÖ **High percentiles matter**‚Äîa single slow backend call can slow down the entire user experience.
- ‚úÖ Track percentiles continuously with **rolling windows** and dashboards.
- ‚úÖ Use **efficient algorithms** like **t-Digest** and **HdrHistogram** instead of sorting all data.
- ‚úÖ Never average percentiles‚Äî**combine histograms** instead.

---
## Understanding Approaches for Coping with Load

As a system grows, it must handle increasing load while maintaining good performance. To achieve this, we need scalability strategies.

### 1. Scaling Up vs. Scaling Out
There are two primary approaches to scaling a system:

#### üîπ Scaling Up (Vertical Scaling)
- üìå **Definition:** Increasing the power of a single machine by upgrading its CPU, RAM, or storage.
- üìå **Advantages:**
  - ‚úÖ Simpler architecture (fewer machines to manage).
  - ‚úÖ Easier to develop and maintain.
- üìå **Disadvantages:**
  - ‚ùå Limited by hardware capabilities (at some point, you can‚Äôt upgrade further).
  - ‚ùå Expensive high-end machines.

#### üí° Example:
- Upgrading from a **4-core CPU** to a **32-core CPU**.

#### üîπ Scaling Out (Horizontal Scaling)
- üìå **Definition:** Adding more machines (servers) and distributing the load across them.
- üìå **Advantages:**
  - ‚úÖ More scalable than vertical scaling (no single machine limit).
  - ‚úÖ Can be cheaper than a single high-end machine.
- üìå **Disadvantages:**
  - ‚ùå More complex architecture (requires load balancing, distributed systems).
  - ‚ùå Potential network latency between machines.

#### üí° Example:
- Instead of upgrading one machine, adding **10 smaller servers** and distributing requests among them.

This approach is also called a **Shared-Nothing Architecture** because each machine operates independently, without shared memory.

---

### 2. Hybrid Approach: The Best of Both Worlds
A good system often combines both approaches:
- üî∏ Use powerful machines where simplicity matters.
- üî∏ Scale out with multiple machines when handling high loads.

#### üí° Example:
- Instead of using 1 supercomputer or 100 tiny servers, use **5-10 powerful machines** for a balance of performance, cost, and manageability.

---

### 3. Manual vs. Elastic Scaling

#### üîπ Manual Scaling
- üìå **Definition:** A human analyzes system load and adds machines manually when needed.
- üìå **Advantages:**
  - ‚úÖ Simpler, predictable behavior.
  - ‚úÖ Less risk of unexpected costs.
- üìå **Disadvantages:**
  - ‚ùå Slow reaction time to traffic spikes.

#### üí° Example:
- A team monitors **CPU usage** and adds more servers when it reaches 80%.

#### üîπ Elastic Scaling (Auto-Scaling)
- üìå **Definition:** The system automatically adds or removes computing resources based on load.
- üìå **Advantages:**
  - ‚úÖ Handles unpredictable spikes automatically.
  - ‚úÖ Saves cost by scaling down when demand is low.
- üìå **Disadvantages:**
  - ‚ùå More complex setup.
  - ‚ùå Can lead to unexpected costs if misconfigured.

#### üí° Example:
- **AWS Auto Scaling** detects increased traffic and adds servers automatically.

---

### 4. Key Takeaways
- ‚úÖ **Scaling Up (Vertical Scaling):** Upgrade a single machine (simple but has limits).
- ‚úÖ **Scaling Out (Horizontal Scaling):** Add more machines (complex but highly scalable).
- ‚úÖ **Hybrid Approach:** Combine both for efficiency.
- ‚úÖ **Manual Scaling:** Simpler but slower to react to changes.
- ‚úÖ **Elastic Scaling:** Automatically adjusts capacity but adds complexity.

---
## Scaling Stateful vs. Stateless Systems

Scaling stateless services is relatively easy‚Äîjust add more machines and distribute the requests. However, scaling stateful systems (e.g., databases) is much more complex because data consistency, availability, and coordination become major challenges.

### 1. Scaling Stateful Systems: Challenges & Approaches

#### üîπ Traditional Approach: Scale Up First
- üìå **Common Wisdom:** Keep the database on a single powerful node until it becomes too costly or unreliable.
- üìå **Why?**
  - ‚úÖ Avoids the complexity of distributed systems.
  - ‚úÖ Ensures strong consistency without complex coordination.

#### üîπ Distributed Databases: The Future?
As distributed system tools improve, more applications are adopting distributed databases early.
- üìå **Possible Future Trend:** Distributed-first approach, even for smaller applications.

#### üí° Example:
- **Cloud-native databases** (e.g., **CockroachDB**, **Spanner**, **YugabyteDB**) are designed for distributed use from the start.

---

### 2. No One-Size-Fits-All Scaling Solution
Each system has different scaling needs based on:
- üìå **Read volume** (e.g., caching needed for high read traffic).
- üìå **Write volume** (e.g., partitioning/sharding for high write traffic).
- üìå **Data size** (e.g., distributed storage for massive datasets).
- üìå **Latency requirements** (e.g., real-time systems need fast response times).

#### üí° Example:
- üî∏ A system handling **100,000 requests/sec**, each **1 kB** needs efficient request distribution (load balancing, caching).
- üî∏ A system handling **3 requests per minute**, each **2 GB** needs optimized storage and batch processing.

> üõë Same data throughput, but completely different scaling architectures!

---

### 3. Assumptions Matter in Scalability Planning
A scalable system is built based on assumptions about:
- üìå **Which operations** are frequent vs. rare?
- üìå **Expected traffic growth patterns?**
- üìå **How data** is accessed and stored?

If assumptions are wrong, scaling efforts can be:
- ‚ùå **Wasted:** Overengineering a problem that doesn‚Äôt exist.
- ‚ùå **Counterproductive:** Introducing unnecessary complexity.

#### üí° Example:
- A **startup** should prioritize rapid iteration on product features over premature optimization for hypothetical large-scale traffic.

---

### 4. Scaling Uses General-Purpose Building Blocks
Even though every system is unique, scalable architectures often rely on common patterns:
- üìå **Caching** (e.g., **Redis**, **Memcached**) ‚Äì To reduce load on databases.
- üìå **Load Balancing** (e.g., **Nginx**, **HAProxy**, **AWS ELB**) ‚Äì To distribute requests across servers.
- üìå **Database Sharding** (e.g., **MySQL**, **PostgreSQL with Citus**) ‚Äì To handle high write loads.
- üìå **Replication** (e.g., **Leader-Follower**, **Multi-Leader**, **Eventual Consistency**) ‚Äì To improve availability and read performance.

#### üí° Key Takeaway:
- Scaling isn‚Äôt about finding a magic solution‚Äîit‚Äôs about choosing the right patterns for your specific use case.

